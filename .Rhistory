# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(x = 1:32, y = y)+
geom_point(x = 1:32, y = pred_lasso, color = 'green')+
geom_point(x = 1:32, y = pred_ridge, color = 'brown')
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = 'green'))+
geom_point(aes(x = 1:32, y = pred_ridge, color = 'brown'))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, fill = pred_lasso))+
geom_point(aes(x = 1:32, y = pred_ridge, color = 'brown'))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, fill = pred_lasso))+
geom_point(aes(x = 1:32, y = pred_ridge, fill = pred_ridge))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = pred_lasso))+
geom_point(aes(x = 1:32, y = pred_ridge, color = pred_ridge))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = 'green''))+
geom_point(aes(x = 1:32, y = pred_ridge, color = 'brown''))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = 'green''))+
geom_point(aes(x = 1:32, y = pred_ridge, color = 'brown))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = 'green''))+
geom_point(aes(x = 1:32, y = pred_ridge, color = ))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = 'green'))+
geom_point(aes(x = 1:32, y = pred_ridge, color = 'brown''))
# if you used glmnet, then you can use the predict function specifying your lambda and a
ex <- glmnet(x = X, y = y, alpha = 1)
pred_ex <- predict(ex, newx = as.matrix(mtcars), s = 0.05)  # newx is the matrix of explanatory variables in your model, and s is the smoothing parameter
# you have to specify a new x
# you can also extract the coefficients from a cv.glmnet
# lets extract the predictions from our optimal LASSO and ridge
pred_lasso <- predict(lam_lasso, newx = as.matrix(mtcars), s = 'lambda.min')
pred_ridge <- predict(lam_ridge, newx = as.matrix(mtcars), s = 'lambda.min')
# can also use an elastic net but must specify your alpha beforehand
ggplot()+
geom_point(aes(x = 1:32, y = y))+
geom_point(aes(x = 1:32, y = pred_lasso, color = 'green'))+
geom_point(aes(x = 1:32, y = pred_ridge, color = 'brown'))
RW <- function(N, x0, mu, variance) {
z<-cumsum(rnorm(n=N, mean=0,
sd=sqrt(variance)))
t<-1:N
x<-x0+t*mu+z
return(x)
}
P1<-RW(1000,0,0,0.5)
plot(1:1000,P1, main="Non-stationary Process Example", ylab="Value",type="l")
ma_data <- arima.sim(model=list(ma=0.9), n=1000)
plot(ma_data, main="Stationary Process Example", ylab="Value")
ar_data <- arima.sim(model=list(ar=0.9), n=1000)
plot(ar_data, main="Stationary Process Example", ylab="Value")
# P1 is the non-stationary time series from above
# ma_data is a stationary time series
# acf is a function that plots auto correlation of a time series
acf(P1)
# P1 is the non-stationary time series from above
# ma_data is a stationary time series
# acf is a function that plots auto correlation of a time series
acf(P1)
acf(ma_data)
acf(P1)
pacf(P1)
acf(P1)
pacf(P1)
acf(ma_data)
pacf(ma_data)
acf(ar_data)
pacf(ar_data)
gc()
RW <- function(N, x0, mu, variance) {
z<-cumsum(rnorm(n=N, mean=0,
sd=sqrt(variance)))
t<-1:N
x<-x0+t*mu+z
return(x)
}
P1<-RW(1000,0,0,0.5)
plot(1:1000,P1, main="Non-stationary Process Example", ylab="Value",type="l")
library(tseries)
RW <- function(N, x0, mu, variance) {
z<-cumsum(rnorm(n=N, mean=0,
sd=sqrt(variance)))
t<-1:N
x<-x0+t*mu+z
return(x)
}
P1<-RW(1000,0,0,0.5)
plot(1:1000,P1, main="Non-stationary Process Example", ylab="Value",type="l")
ma_data <- arima.sim(model=list(ma=0.9), n=1000)
plot(ma_data, main="Stationary Pro
cess Example", ylab="Value")
ar_data <- arima.sim(model=list(ar=0.9), n=1000)
plot(ar_data, main="Stationary Process Example", ylab="Value")
# P1 is the non-stationary time series from above
# ma_data is a stationary time series
# acf is a function that plots auto correlation of a time series
acf(P1)
acf(ma_data)
acf(P1)
pacf(P1)
acf(ma_data)
pacf(ma_data)
acf(ar_data)
pacf(ar_data)
library(tseries)
adf.test(P1)
library(tseries)
adf.test(P1) # p = 0.477 > 0.05 reject H0 and conclude there is not statistically significant evidence that this time series is stationary.
adf.test(ma_data)
library(tseries)
adf.test(P1) # p = 0.477 > 0.05 fail to reject H0 and conclude there is not statistically significant evidence that this time series is stationary.
adf.test(ma_data) # p < 0.05, so we reject H0 and conclude there is statistically significant evidence that the time series is stationary.
library(tseries)
adf.test(P1) # p = 0.477 > 0.05 fail to reject H0 and conclude there is not statistically significant evidence that this time series is stationary.
adf.test(ma_data) # p < 0.05, so we reject H0 and conclude there is statistically significant evidence that the time series is stationary.
adf.test(ar_data)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
arma(dat)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
mod <- arma(dat)
summary(mod)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
mod <- arma(dat)
summary(mod)
# because we are assuming normality, we can calculate confidence intervals for the given data.
confint(mod)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
mod <- arma(dat)
summary(mod)
# because we are assuming normality, we can calculate confidence intervals for the given data.
confint(mod)
# the estimates and confidence intervals for these parameters represent correlation structures for the time series data, and the confidence interval calculation is the same as multiple linear regression!
# let's see what happens when we fit an ARMA(1,2) model
mod_2 <- arma(dat, orderz=c(1,2))
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
mod <- arma(dat)
summary(mod)
# because we are assuming normality, we can calculate confidence intervals for the given data.
confint(mod)
# the estimates and confidence intervals for these parameters represent correlation structures for the time series data, and the confidence interval calculation is the same as multiple linear regression!
# let's see what happens when we fit an ARMA(1,2) model
mod_2 <- arma(dat, order=c(1,2))
summary(mod_2)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
mod <- arma(dat)
summary(mod)
# because we are assuming normality, we can calculate confidence intervals for the given data.
confint(mod)
# the estimates and confidence intervals for these parameters represent correlation structures for the time series data, and the confidence interval calculation is the same as multiple linear regression!
# let's see what happens when we fit an ARMA(1,2) model
mod_2 <- arma(dat, order=c(1,3))
summary(mod_2)
# Simulate the dataset
## arima.sim simulates a stationary time series dataset
## AR1 parameter 0.7 (beta1)
## MA1 parameter 0.5 (theta1)
set.seed(100618)
dat <- arima.sim(model=list(ar = 0.7, ma=0.5), n=1000)
plot(dat)
# in the package tseries, the arma() function will fit an arma model for a specific p and q.
mod <- arma(dat)
summary(mod)
# because we are assuming normality, we can calculate confidence intervals for the given data.
confint(mod)
# the estimates and confidence intervals for these parameters represent correlation structures for the time series data, and the confidence interval calculation is the same as multiple linear regression!
# let's see what happens when we fit an ARMA(1,2) model
mod_2 <- arma(dat, order=c(1,2))
summary(mod_2)
anova(mod, mod_2)
airpassengers
Airpassengers
AirPassengers
AirPassengers
plot(AirPassengers)
plot(AirPassengers)
adf.test(AirPassengers)
adf.test(AirPassengers)
# small p value means the test indicates evidence that the data is stationary
# lets look and see if the data fit an AR1 model
pacf(AirPassengers)
mod_1 <- arma(AirPassengers, order = c(1,0))
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
summary(mod_2)
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
summary(mod_2)
mod_3 <- arma(AirPassengers, order = c(1,2))
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
summary(mod_2)
mod_3 <- arma(AirPassengers, order = c(1,2))
summary(mod_3)
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
summary(mod_2)
#if we are concerned about nonstationarity, we can try ARIMA(I stands for integration), which is a modeling structure for non stationary time series
mod_arima <- arima(AirPassengers, order = c(1,1,1)
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
summary(mod_2)
#if we are concerned about nonstationarity, we can try ARIMA(I stands for integration), which is a modeling structure for non stationary time series
mod_arima <- arima(AirPassengers, order = c(1,1,1))
mod_1 <- arma(AirPassengers, order = c(1,0))
summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
summary(mod_2)
#if we are concerned about nonstationarity, we can try ARIMA(I stands for integration), which is a modeling structure for non stationary time series
mod_arima <- arima(AirPassengers, order = c(1,1,1))
summary(mod_arima)
mod_1 <- arma(AirPassengers, order = c(1,0))
#summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
#summary(mod_2)
#if we are concerned about nonstationarity, we can try ARIMA(I stands for integration), which is a modeling structure for non stationary time series
mod_arima <- arima(AirPassengers, order = c(1,1,1))
summary(mod_arima)
mod_1 <- arma(AirPassengers, order = c(1,0))
#summary(mod_1)
mod_2 <- arma(AirPassengers, order = c(1,1))
#summary(mod_2)
#if we are concerned about nonstationarity, we can try ARIMA(I stands for integration), which is a modeling structure for non stationary time series
mod_arima <- arima(AirPassengers, order = c(1,1,1))
summary(mod_arima)
coef(mod_arima)
vcov(mod_arima)
P_D <- 0.02
P_PD <- 0.99
P_NDc <- 0.95
P_P <- P_PD * P_D + (1 - P_NDc) * (1 - P_D)
P_D <- 0.02
P_PD <- 0.99
P_NDc <- 0.95
P_P <- P_PD * P_D + (1 - P_NDc) * (1 - P_D)
P_PD
P_D <- 0.02
P_PD <- 0.99
P_NDc <- 0.95
P_P <- P_PD * P_D + (1 - P_NDc) * (1 - P_D)
P_PD * P_D / P_P
library(shiny)
library(ggplot2)
# UI
ui <- fluidPage(
titlePanel("b-Binomial Prior vs Posterior"),
sidebarLayout(
sidebarPanel(
h3("Prior Parameters"),
numericInput("a", "a", value = 2, min = 0.1, step = 0.1),
numericInput("b", "b", value = 2, min = 0.1, step = 0.1),
hr(),
h3("Observed Data"),
numericInput("n", "Number of Trials (n)", value = 10, min = 1, step = 1),
numericInput("k", "Number of Successes (x)", value = 5, min = 0, step = 1),
hr(),
actionButton("update", "Update")
),
mainPanel(
plotOutput("distPlot")
)
)
)
# Server
server <- function(input, output) {
reactive_values <- reactiveValues()
observeEvent(input$update, {
# Capture inputs
a_prior <- input$a
b_prior <- input$b
n <- input$n
k <- input$k
# Posterior parameters
a_post <- a_prior + k
b_post <- b_prior + n - k
# Data for visualization
x <- seq(0, 1, length.out = 500)
prior <- dbeta(x, a_prior, b_prior)
posterior <- dbeta(x, a_post, b_post)
reactive_values$data <- data.frame(
x = c(x, x),
y = c(prior, posterior),
Distribution = rep(c("Prior", "Posterior"), each = length(x))
)
})
output$distPlot <- renderPlot({
req(reactive_values$data)
ggplot(reactive_values$data, aes(x = x, y = y, color = Distribution)) +
geom_line(size = 1.2) +
labs(
title = "Prior vs Posterior Distribution",
x = "Probability of Success (θ)",
y = "Density"
) +
theme_minimal() +
theme(legend.title = element_blank())
})
output$posteriorParams <- renderText({
req(reactive_values$posteriorParams)
reactive_values$posteriorParams
})
}
# Run the app
shinyApp(ui = ui, server = server)
library(shiny)
library(ggplot2)
# UI
ui <- fluidPage(
titlePanel("b-Binomial Prior vs Posterior"),
sidebarLayout(
sidebarPanel(
h3("Prior Parameters"),
numericInput("a", "a", value = 2, min = 0.1, step = 0.1),
numericInput("b", "b", value = 2, min = 0.1, step = 0.1),
hr(),
h3("Observed Data"),
numericInput("n", "Number of Trials (n)", value = 10, min = 1, step = 1),
numericInput("k", "Number of Successes (x)", value = 5, min = 0, step = 1),
hr(),
actionButton("update", "Update")
),
mainPanel(
plotOutput("distPlot")
)
)
)
# Server
server <- function(input, output) {
reactive_values <- reactiveValues()
observeEvent(input$update, {
# Capture inputs
a_prior <- input$a
b_prior <- input$b
n <- input$n
k <- input$k
# Posterior parameters
a_post <- a_prior + k
b_post <- b_prior + n - k
# Data for visualization
x <- seq(0, 1, length.out = 500)
prior <- dbeta(x, a_prior, b_prior)
posterior <- dbeta(x, a_post, b_post)
reactive_values$data <- data.frame(
x = c(x, x),
y = c(prior, posterior),
Distribution = rep(c("Prior", "Posterior"), each = length(x))
)
})
output$distPlot <- renderPlot({
req(reactive_values$data)
ggplot(reactive_values$data, aes(x = x, y = y, color = Distribution)) +
geom_line(size = 1.2) +
labs(
title = "Prior vs Posterior Distribution",
x = "Probability of Success (θ)",
y = "Density"
) +
theme_minimal() +
theme(legend.title = element_blank())
})
output$posteriorParams <- renderText({
req(reactive_values$posteriorParams)
reactive_values$posteriorParams
})
}
# Run the app
shinyApp(ui = ui, server = server)
