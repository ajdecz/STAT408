---
title: "Transformations and Polynomial Regression"
format: 
  html:
    self-contained: true
    embed-resources: true
editor: visual
execute: 
  echo: true
  include: true
---

## Motivating Example

Consider the dataset *Boston* available in the package *MASS*.

```{r}
library(MASS)
# The Boston dataset is inside the MASS package

```

We want to create a model to predict the median value of owner-occupied homes.

With multiple linear regression, we have discussed the various assumptions needed to perform regression analysis. Combining these assumptions, with a response variable $Y$ and $k-1$ explanatory variables, $X_1,X_2,\dots,X_{k-1}$, we have that $$Y \sim \mathcal{N}\left(\beta_0 + \beta_1X_1 + \cdots + \beta_{k-1}X_{k-1},\sigma^2\right),$$ where we assume that all of the values are normally distributed with a linear mean structure and common variance. This can also include interaction terms.

Let's check the assumptions for the linear model predicting median value of homes in Boston suburbs with all of the other variables in the dataset as well as interaction terms.

```{r}
mod <- lm(medv ~ ., Boston)
library(lmtest)
plot(mod,1)
plot(mod,2)
bptest(mod)
ks.test(rstandard(mod), "pnorm")
```

based on the plots and the numeric tests, both plots are violated.

**Transformation**: Inputting the response variable ($Y$) into a function to obtain a new response ($\tilde{Y}$) that more closely meets the assumptions for a linear regression model.

Common Types of transformations of a response variable:

-   Log-Transformation ($\tilde{Y} = \log(Y), Y > 0$): (Note: when I say log, I am referring to the natural logarithm)

    -   stabilizes the variance if the variability increases significantly with any $X$ (witnessing a horn shape in the residual plot)
    -   normalizes the variable if it is highly right skewed (many points on the upper end of QQ-plot are above the 45-degree line)
    -   linearizes the relationship if the relationship between $X$ and $Y$ appears to be exponential (look at this via a scatterplot)

-   Square Root-Transformation ($\tilde{Y} = \sqrt{Y}, Y \geq 0$):

    -   stabilizes the variance if the variance is proportional to the mean (i.e. $\text{sd}(Y | X_1,\dots,X_p) = c\text{E}(Y | X_1,\dots,X_p)$ where $c$ is a constant) . This can be discovered via a box-cox plot.

-   Squared-Transformation ($\tilde{Y} = Y^2$):

    -   stabilizes the variance if the variance is decreases significantly with any $X$ (horn is pointing left in the residual plot)
    -   normalizes the variable if ii tis highly right skewed (many points are on the upper end of qq-plot are above the 45 degree line). this is what we see in the qq plot for the boston dataset.

So now, the model we are fitting is

$$\tilde{Y} \sim \mathcal{N}\left(\beta_0 + \beta_1X_1 + \cdots + \beta_{k-1}X_{k-1},\sigma^2\right),$$

```         
The method of finding the regression estimates 
$\hat{\beta}_0,\hat{\beta}_1,\dots,\hat{\beta}_{k-1}$
 is still minimizing the sum of squared errors:
```

\$\\hat{\\beta}\_0,\\hat{\\beta}\_1,\\dots,\\hat{\\beta}\_{k-1}\$

$$\sum_{i=1}^n \left(\tilde{y}_i - (\hat{\beta}_0 + \hat{\beta}_1x_{i1} + \cdots + \hat{\beta}_{k-1}x_{i,k-1})\right)^2$$

### Example

Consider the Boston dataset once again.

Perform a least squares regression analysis for the natural log of median home value with crime rate as the only explanatory variable by answering the following questions:

a.  Write out the least squares regression line

```{r}
mod_log <- lm(log(medv) ~ crim, Boston)
coef(mod_log)
```

b\. $\hat{log(medV)} = 3.125 - 0.02509 \times crim$

c\. Interpret the slope of the least squares regression line in the context of the given problem.

-0.02509: for a one percent increase in the per capita crime rate, the expected log median home value decreases by 0.02509 log-thousand dollars

OR

For a one percent increase in the per capita crime rate the expected median home value changes by a factor of exp(-0.02509) = 0.9752

d\. Perform an F-test to determine if this log-linear (because log transformation) model for sales price is statistically significant. #not to test if the transformed model is better, testing to see if there is a significant linear relationship within the transformed model

Reduced Model: $\hat{log(medv)} = \beta_0$

Full Model: $\hat{log(medv)} = \beta_0 + \beta_1 crim$

$H_0: \beta_1 = 0$

$H_a: \beta_1 \neq 0$

```{r}
summary(mod_log)
```

d.  $F = 198.4, p < 2.2 \times 10^{-16}$ so we reject null hypothesis that crime rate is statistically significant in predicting median home value in Boston suburbs through a log linear model.
e.  
f.  Check to see if the assumptions of homoscedasticity and normally distributed residuals are met for this transformed linear model.

```{r}
```

**Polynomial Regression**: Extension of a linear regression model where we now allow for quadratic, cubic, and higher-order terms in our regression model. If we have a single explanatory variable $X$, then we can have $$Y \sim \mathcal{N}\left(\beta_0 + \beta_1 X + \beta_2 X^2 + \dots + \beta_k X^k,\sigma^2\right)$$ or if $Y$ is transformed $$\tilde{Y} \sim \mathcal{N}\left(\beta_0 + \beta_1 X + \beta_2 X^2 + \dots + \beta_k X^k,\sigma^2\right)$$

The method of finding the regression estimates $\hat{\beta}_0,\hat{\beta}_1,\dots,\hat{\beta}_k$ is still minimizing the sum of squared errors:

$$\sum_{i=1}^n \left(y_i - (\hat{\beta}_0 + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_k x_i^k)\right)^2$$

### Example

Perform a least squares regression analysis for log median home value with a linear and quadratic term for crime rate by answering the following questions:

a.  Write out the least squares regression line:

```{r}
```

b.  Perform an F-test to determine if a polynomial model with a linear and quadratic term for crime rate significantly improves the predictive ability of log median home value (with nothing else).

```{r}
```

c.  Perform an F-test to determine if adding a quadratic term to a least squares regression line which already includes a linear term significantly improves the predictive ability of log median home value.

```{r}
```

d.  Check to see if the assumptions of homoscedasticity and normally distributed residuals are met for this transformed polynomial model.

```{r}
```
